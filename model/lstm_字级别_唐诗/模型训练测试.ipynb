{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8d8a9a72-8677-480c-b3c3-f98706a30c77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "import pickle\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbc15a3f-5507-4de7-b9ad-1774b1f16ed9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.1+cu121\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39ccbbdb-ba04-49f7-af41-322a6b15a17c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文件已成功读取，共 812548 行。\n",
      "文件已成功读取，共 815991 行。\n",
      "文件已成功读取，共 1599430 行。\n"
     ]
    }
   ],
   "source": [
    "text_list = []\n",
    "def read_text_to_list(filepath,lines):\n",
    "    \"\"\"\n",
    "    读取文本文件，将每一行作为一项保存到列表中。\n",
    "\n",
    "    :param filepath: 文本文件路径\n",
    "    :return: 包含文件每一行内容的列表\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 打开文件，使用 'r' 模式读取，指定编码为 UTF-8\n",
    "        with open(filepath, 'r', encoding='utf-8') as file:\n",
    "            # 逐行读取文件内容\n",
    "            for line in file:\n",
    "                lines.append(line.strip())  # 去掉行末的换行符并添加到列表\n",
    "        print(f\"文件已成功读取，共 {len(lines)} 行。\")\n",
    "    except Exception as e:\n",
    "        print(f\"读取文件时出错：{e}\")\n",
    "\n",
    "read_text_to_list('../../dataset/data_pro/唐诗/七言唐诗.txt',text_list)\n",
    "read_text_to_list('../../dataset/data_pro/唐诗/六言唐诗.txt',text_list)\n",
    "read_text_to_list('../../dataset/data_pro/唐诗/五言唐诗.txt',text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "494c62e4-b73b-4ee0-847d-8ae5fef51da9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "label_text,target_text = [],[]\n",
    "for i in text_list:\n",
    "    label_text.append(i.split()[0])\n",
    "    target_text.append(i.split()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a70e21ea-9fb8-4660-a679-e96b4787dd6a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['少年力学志须彊',\n",
       " '试问邯郸欹枕客',\n",
       " '脂脸轻匀作艳粧',\n",
       " '夭红不见凌霜操',\n",
       " '纷纷朝市竞秋毫',\n",
       " '不问扬澜与彭浪',\n",
       " '似闻疏雨打篷声',\n",
       " '明日觉来浑不记',\n",
       " '夹屋青松翠霭中',\n",
       " '重来乌石冈头路']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_text[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44ab29e3-5e1c-4de3-adeb-df036162cba9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['得失由来一梦长',\n",
       " '人间几度熟黄粱',\n",
       " '未应洁白似梅香',\n",
       " '漫向春前取次芳',\n",
       " '江上霜风正怒号',\n",
       " '翩然东下日千艘',\n",
       " '枕上悠扬梦半醒',\n",
       " '隔船相语过前汀',\n",
       " '去年经此亦匆匆',\n",
       " '依旧松声带晓风']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_text[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ab8fd51-819a-4cff-bc1f-460a56cc461e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 构建词汇表\n",
    "def yield_tokens(data):\n",
    "    for text in data:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "\n",
    "# 初始化分词器（字符级别）\n",
    "tokenizer = get_tokenizer(None)  # 默认按字符分词\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(label_text + target_text), specials=[\"<unk>\", \"<pad>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "\n",
    "# 将文本转换为索引序列\n",
    "label_sequences = [torch.tensor(vocab(tokenizer(text)), dtype=torch.long) for text in label_text]\n",
    "target_sequences = [torch.tensor(vocab(tokenizer(text)), dtype=torch.long) for text in target_text]\n",
    "\n",
    "# 填充序列，使它们具有相同的长度\n",
    "label_sequences = pad_sequence(label_sequences, batch_first=True, padding_value=vocab[\"<pad>\"])\n",
    "target_sequences = pad_sequence(target_sequences, batch_first=True, padding_value=vocab[\"<pad>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "187ec3be-f706-49f5-b4e6-959637218bed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10203"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0bf9795-4b1d-49b3-b170-953a9e7c27ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 保存 vocab 到文件\n",
    "with open('./vocab.pkl', 'wb') as f:\n",
    "    pickle.dump(vocab, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95db01db-6a49-4a49-99e8-7c92d03fc4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从文件加载 vocab\n",
    "with open('./model/v1.0_lstm_单字/vocab.pkl', 'rb') as f:\n",
    "    vocab = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "27ed78b9-c6df-4834-b8aa-27ec14f0c6a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, label_sequences, target_sequences):\n",
    "        self.label_sequences = label_sequences\n",
    "        self.target_sequences = target_sequences\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.label_sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.label_sequences[idx], self.target_sequences[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4ed9a30a-44da-4e4d-a44c-00f78b6f4c45",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1599430"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(label_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "692432a7-0d94-40ab-a4c4-9414cbedcf3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = TextDataset(label_text, target_text)\n",
    "dataloader = DataLoader(dataset, batch_size=600000, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e024c9f3-2004-44c7-831a-d634f97a083d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMModel(\n",
      "  (embedding): Embedding(10203, 64)\n",
      "  (lstm): LSTM(64, 128, batch_first=True)\n",
      "  (fc): Linear(in_features=128, out_features=10203, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "        output = self.fc(lstm_out)\n",
    "        return output\n",
    "\n",
    "# 模型参数\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 64\n",
    "hidden_dim = 128\n",
    "\n",
    "# 初始化模型\n",
    "model = LSTMModel(vocab_size, embedding_dim, hidden_dim)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "# 打印模型结构\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9620f9-d52f-4ba0-948c-47008481da0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# 损失函数和优化器\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=vocab[\"<pad>\"])  # 忽略填充部分\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 训练循环\n",
    "num_epochs = 100\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 前向传播\n",
    "    outputs = model(input_sequences)\n",
    "    loss = criterion(outputs.view(-1, vocab_size), target_sequences.view(-1))\n",
    "\n",
    "    # 反向传播和优化\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 打印损失\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c11730-99f5-42af-add9-46b696bfb22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 调整批次大小\n",
    "batch_size = 32\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# 假设你已经定义好了模型\n",
    "model = YourModel().to(device)  # 将模型移动到GPU上\n",
    "\n",
    "# 损失函数和优化器\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=vocab[\"<pad>\"])  # 忽略填充部分\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 训练循环\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "4\n",
    "    for batch_input, batch_target in dataloader:\n",
    "        # 将数据移动到GPU上\n",
    "        batch_input, batch_target = batch_input.to(device), batch_target.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 前向传播\n",
    "        outputs = model(batch_input)\n",
    "        loss = criterion(outputs.view(-1, vocab_size), batch_target.view(-1))\n",
    "\n",
    "        # 反向传播和优化\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # 打印损失\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
