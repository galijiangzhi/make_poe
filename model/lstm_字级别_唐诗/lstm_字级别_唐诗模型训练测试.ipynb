{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e449d379-c3e6-486e-9079-49d99fc4c856",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# from torchtext.data.utils import get_tokenizer\n",
    "# from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "import pickle\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c7df96b-da5a-42a1-951a-21f30a1845a0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文件已成功读取，共 812548 行。\n",
      "文件已成功读取，共 815991 行。\n",
      "文件已成功读取，共 1599430 行。\n"
     ]
    }
   ],
   "source": [
    "# # 数据准备\n",
    "# input_texts = [\"床前明月光\", \"举头望明月\", \"千山鸟飞绝\"]\n",
    "# target_texts = [\"疑是地上霜\", \"低头思故乡\", \"万径人踪灭\"]\n",
    "text_list = []\n",
    "def read_text_to_list(filepath,lines):\n",
    "    \"\"\"\n",
    "    读取文本文件，将每一行作为一项保存到列表中。\n",
    "\n",
    "    :param filepath: 文本文件路径\n",
    "    :return: 包含文件每一行内容的列表\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 打开文件，使用 'r' 模式读取，指定编码为 UTF-8\n",
    "        with open(filepath, 'r', encoding='utf-8') as file:\n",
    "            # 逐行读取文件内容\n",
    "            for line in file:\n",
    "                lines.append(line.strip())  # 去掉行末的换行符并添加到列表\n",
    "        print(f\"文件已成功读取，共 {len(lines)} 行。\")\n",
    "    except Exception as e:\n",
    "        print(f\"读取文件时出错：{e}\")\n",
    "\n",
    "read_text_to_list('../../dataset/data_pro/唐诗/七言唐诗.txt',text_list)\n",
    "read_text_to_list('../../dataset/data_pro/唐诗/六言唐诗.txt',text_list)\n",
    "read_text_to_list('../../dataset/data_pro/唐诗/五言唐诗.txt',text_list)\n",
    "\n",
    "input_texts,target_texts = [],[]\n",
    "for i in text_list:\n",
    "    input_texts.append(i.split()[0])\n",
    "    target_texts.append(i.split()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f482f81-693f-4872-a53f-58404ad5b2db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 创建字符到索引的映射\n",
    "chars = list(set(\"\".join(input_texts + target_texts)))\n",
    "pad =list({\"<PAD>\",\"<EOS>\"})\n",
    "chars = pad+chars  # 使用集合的并集操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21d3cfb6-d950-41ee-a68e-f8755aa92554",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# char_to_idx = {char: idx for idx, char in enumerate(chars)}\n",
    "# idx_to_char = {idx: char for char, idx in char_to_idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a64f889-9272-429b-913c-f4d9e865cfb5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# with open('char_to_idx_1.pkl', 'wb') as f:\n",
    "#     pickle.dump(char_to_idx, f)\n",
    "\n",
    "# # 保存 idx_to_char 到文件\n",
    "# with open('idx_to_char_1.pkl', 'wb') as f:\n",
    "#     pickle.dump(idx_to_char, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "11a01ed1-64d9-4175-9ef9-216cf63e99c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 读取 char_to_idx\n",
    "with open('char_to_idx_1.pkl', 'rb') as f:\n",
    "    char_to_idx = pickle.load(f)\n",
    "\n",
    "# 读取 idx_to_char\n",
    "with open('idx_to_char_1.pkl', 'rb') as f:\n",
    "    idx_to_char = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "32f170b2-0ea8-433f-ad91-404808e56179",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 将文本转换为索引序列\n",
    "def text_to_seq(text, char_to_idx):\n",
    "    return [char_to_idx.get(char,random.randint(2, len(char_to_idx)-1)) for char in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "de28d1e3-f639-4152-bb92-9ebcef8fd88b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_seqs = [text_to_seq(text, char_to_idx) for text in input_texts]\n",
    "target_seqs = [text_to_seq(text, char_to_idx) for text in target_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5d8da886-f1ae-4bc2-9b01-2cab31c9f922",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1975, 1723, 3163, 476, 1839, 2291, 2529]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_seqs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ff34e75-0e07-4986-9f02-902c950315a9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1975, 1723, 3163, 476, 1839, 2291, 2529]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_seqs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "13e2cb96-33a6-4287-b0a2-5659fc47608b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 填充序列到相同长度\n",
    "max_len = 10\n",
    "# max_len = max(len(seq) for seq in input_seqs + target_seqs)\n",
    "input_seqs = [seq +[1] + [0] * (max_len - len(seq) -1) for seq in input_seqs]\n",
    "target_seqs = [seq + [1] + [0] * (max_len - len(seq) -1) for seq in target_seqs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99226ece-7039-4a6f-8d67-35fb7bc18ec4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, label_sequences, target_sequences):\n",
    "        self.label_sequences =  torch.tensor(label_sequences, dtype=torch.long)\n",
    "        self.target_sequences = torch.tensor(target_sequences, dtype=torch.long)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.label_sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.label_sequences[idx], self.target_sequences[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "62d90947-4cfc-44e9-a16e-db15b6eaafdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_tensor = torch.tensor(input_seqs, dtype=torch.long)\n",
    "# target_tensor = torch.tensor(target_seqs, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f283b45-285d-4557-b9eb-b707f32bd611",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = TextDataset(input_seqs, target_seqs)\n",
    "dataloader = DataLoader(dataset, batch_size=80000, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "69243017-1aa3-4ec8-a519-3e59242b4051",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "testset = TextDataset(input_seqs[0:50], target_seqs[0:50])\n",
    "dataloader = DataLoader(testset, batch_size=10, shuffle=True)\n",
    "# input_tests = input_tests[0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fa9eda21-1554-46d7-a826-24551d317610",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 定义Seq2Seq模型\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.encoder = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
    "        self.decoder = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        _, (hidden, cell) = self.encoder(embedded)\n",
    "        output, _ = self.decoder(embedded, (hidden, cell))\n",
    "        output = self.fc(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "657c2eca-0e5d-4d03-881d-ac06e7f24529",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 模型参数\n",
    "input_size = len(chars)\n",
    "hidden_size = 64\n",
    "output_size = len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4ed0e3f1-4df1-4f4c-a46e-e84f6090bfb6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (embedding): Embedding(3865, 64)\n",
       "  (encoder): LSTM(64, 64, batch_first=True)\n",
       "  (decoder): LSTM(64, 64, batch_first=True)\n",
       "  (fc): Linear(in_features=64, out_features=3865, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 初始化模型、损失函数和优化器\n",
    "model = Seq2Seq(input_size, hidden_size, output_size)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "86d4bdb7-3892-478a-9776-82b13c77d07e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (embedding): Embedding(3865, 64)\n",
       "  (encoder): LSTM(64, 64, batch_first=True)\n",
       "  (decoder): LSTM(64, 64, batch_first=True)\n",
       "  (fc): Linear(in_features=64, out_features=3865, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.load('./model_seq2seq_h64.pth')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3881dfd7-80c3-4fe5-9fef-b4329a0bc661",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[0;32m----> 2\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(\u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e82f3871-ff0f-4317-a363-c10b999c1e6c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Loss: 0.1337\n"
     ]
    }
   ],
   "source": [
    "# 训练模型\n",
    "epochs = 1\n",
    "losss = 15\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    for input_tensor, target_tensor in dataloader:\n",
    "        input_tensor = input_tensor.to(device)\n",
    "        target_tensor = target_tensor.to(device)\n",
    "        output = model(input_tensor)\n",
    "        loss = criterion(output.view(-1, output_size), target_tensor.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # if (epoch + 1) % 10 == 0:\n",
    "        #     print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\")\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\")\n",
    "    if loss.item() < losss:\n",
    "        losss = loss.item()\n",
    "        torch.save(model, 'model_seq2seq_h64.pth')\n",
    "        # print(f\"Loss: {loss.item():.4f}，效果更好，进行保存\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92935330-3f8c-48a1-bcb0-b831fe973af9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(model, 'model_seq2seq_h64.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbf7464-f049-4a4d-86c5-28bb0bb15da1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eb615c1e-0b00-4118-bd08-7f5b15102716",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_size = len(chars)\n",
    "hidden_size =128\n",
    "output_size = len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "27f4246a-030f-42a8-b8df-10ee4f882be0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 初始化模型、损失函数和优化器\n",
    "model = Seq2Seq(input_size, hidden_size, output_size)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ff1cd772-8076-46cb-8ad6-42b617f69186",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75083e78-a94e-4473-a41e-979c1db3df77",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2000], Loss: 6.0268\n",
      "Loss: 6.0268，效果更好，进行保存\n",
      "Epoch [2/2000], Loss: 4.7342\n",
      "Loss: 4.7342，效果更好，进行保存\n",
      "Epoch [3/2000], Loss: 4.4887\n",
      "Loss: 4.4887，效果更好，进行保存\n",
      "Epoch [4/2000], Loss: 4.3923\n",
      "Loss: 4.3923，效果更好，进行保存\n",
      "Epoch [5/2000], Loss: 4.3086\n",
      "Loss: 4.3086，效果更好，进行保存\n",
      "Epoch [6/2000], Loss: 4.2662\n",
      "Loss: 4.2662，效果更好，进行保存\n",
      "Epoch [7/2000], Loss: 4.2190\n",
      "Loss: 4.2190，效果更好，进行保存\n",
      "Epoch [8/2000], Loss: 4.1741\n",
      "Loss: 4.1741，效果更好，进行保存\n",
      "Epoch [9/2000], Loss: 4.1307\n",
      "Loss: 4.1307，效果更好，进行保存\n",
      "Epoch [10/2000], Loss: 4.0865\n",
      "Loss: 4.0865，效果更好，进行保存\n",
      "Epoch [11/2000], Loss: 4.0340\n",
      "Loss: 4.0340，效果更好，进行保存\n",
      "Epoch [12/2000], Loss: 3.9992\n",
      "Loss: 3.9992，效果更好，进行保存\n",
      "Epoch [13/2000], Loss: 3.9539\n",
      "Loss: 3.9539，效果更好，进行保存\n",
      "Epoch [14/2000], Loss: 3.9336\n",
      "Loss: 3.9336，效果更好，进行保存\n",
      "Epoch [15/2000], Loss: 3.8856\n",
      "Loss: 3.8856，效果更好，进行保存\n",
      "Epoch [16/2000], Loss: 3.7783\n",
      "Loss: 3.7783，效果更好，进行保存\n",
      "Epoch [17/2000], Loss: 3.6825\n",
      "Loss: 3.6825，效果更好，进行保存\n",
      "Epoch [18/2000], Loss: 3.6002\n",
      "Loss: 3.6002，效果更好，进行保存\n",
      "Epoch [19/2000], Loss: 3.5009\n",
      "Loss: 3.5009，效果更好，进行保存\n",
      "Epoch [20/2000], Loss: 3.4001\n",
      "Loss: 3.4001，效果更好，进行保存\n",
      "Epoch [21/2000], Loss: 3.3180\n",
      "Loss: 3.3180，效果更好，进行保存\n",
      "Epoch [22/2000], Loss: 3.2503\n",
      "Loss: 3.2503，效果更好，进行保存\n",
      "Epoch [23/2000], Loss: 3.1891\n",
      "Loss: 3.1891，效果更好，进行保存\n",
      "Epoch [24/2000], Loss: 3.0863\n",
      "Loss: 3.0863，效果更好，进行保存\n",
      "Epoch [25/2000], Loss: 3.0073\n",
      "Loss: 3.0073，效果更好，进行保存\n",
      "Epoch [26/2000], Loss: 2.8998\n",
      "Loss: 2.8998，效果更好，进行保存\n",
      "Epoch [27/2000], Loss: 2.7958\n",
      "Loss: 2.7958，效果更好，进行保存\n",
      "Epoch [28/2000], Loss: 2.7088\n",
      "Loss: 2.7088，效果更好，进行保存\n",
      "Epoch [29/2000], Loss: 2.6307\n",
      "Loss: 2.6307，效果更好，进行保存\n",
      "Epoch [30/2000], Loss: 2.5576\n",
      "Loss: 2.5576，效果更好，进行保存\n",
      "Epoch [31/2000], Loss: 2.5179\n",
      "Loss: 2.5179，效果更好，进行保存\n",
      "Epoch [32/2000], Loss: 2.4449\n",
      "Loss: 2.4449，效果更好，进行保存\n",
      "Epoch [33/2000], Loss: 2.3824\n",
      "Loss: 2.3824，效果更好，进行保存\n",
      "Epoch [34/2000], Loss: 2.3028\n",
      "Loss: 2.3028，效果更好，进行保存\n",
      "Epoch [35/2000], Loss: 2.2414\n",
      "Loss: 2.2414，效果更好，进行保存\n",
      "Epoch [36/2000], Loss: 2.1791\n",
      "Loss: 2.1791，效果更好，进行保存\n",
      "Epoch [37/2000], Loss: 2.1105\n",
      "Loss: 2.1105，效果更好，进行保存\n",
      "Epoch [38/2000], Loss: 2.0510\n",
      "Loss: 2.0510，效果更好，进行保存\n",
      "Epoch [39/2000], Loss: 2.0038\n",
      "Loss: 2.0038，效果更好，进行保存\n",
      "Epoch [40/2000], Loss: 1.9555\n",
      "Loss: 1.9555，效果更好，进行保存\n",
      "Epoch [41/2000], Loss: 1.9199\n",
      "Loss: 1.9199，效果更好，进行保存\n",
      "Epoch [42/2000], Loss: 1.8558\n",
      "Loss: 1.8558，效果更好，进行保存\n",
      "Epoch [43/2000], Loss: 1.7994\n",
      "Loss: 1.7994，效果更好，进行保存\n",
      "Epoch [44/2000], Loss: 1.7561\n",
      "Loss: 1.7561，效果更好，进行保存\n",
      "Epoch [45/2000], Loss: 1.7049\n",
      "Loss: 1.7049，效果更好，进行保存\n",
      "Epoch [46/2000], Loss: 1.6712\n",
      "Loss: 1.6712，效果更好，进行保存\n",
      "Epoch [47/2000], Loss: 1.6297\n",
      "Loss: 1.6297，效果更好，进行保存\n",
      "Epoch [48/2000], Loss: 1.5911\n",
      "Loss: 1.5911，效果更好，进行保存\n",
      "Epoch [49/2000], Loss: 1.5584\n",
      "Loss: 1.5584，效果更好，进行保存\n",
      "Epoch [50/2000], Loss: 1.5051\n",
      "Loss: 1.5051，效果更好，进行保存\n",
      "Epoch [51/2000], Loss: 1.4589\n",
      "Loss: 1.4589，效果更好，进行保存\n",
      "Epoch [52/2000], Loss: 1.4248\n",
      "Loss: 1.4248，效果更好，进行保存\n",
      "Epoch [53/2000], Loss: 1.3975\n",
      "Loss: 1.3975，效果更好，进行保存\n",
      "Epoch [54/2000], Loss: 1.3699\n",
      "Loss: 1.3699，效果更好，进行保存\n",
      "Epoch [55/2000], Loss: 1.3459\n",
      "Loss: 1.3459，效果更好，进行保存\n",
      "Epoch [56/2000], Loss: 1.3337\n",
      "Loss: 1.3337，效果更好，进行保存\n",
      "Epoch [57/2000], Loss: 1.3151\n",
      "Loss: 1.3151，效果更好，进行保存\n",
      "Epoch [58/2000], Loss: 1.2970\n",
      "Loss: 1.2970，效果更好，进行保存\n",
      "Epoch [59/2000], Loss: 1.2523\n",
      "Loss: 1.2523，效果更好，进行保存\n",
      "Epoch [60/2000], Loss: 1.2161\n",
      "Loss: 1.2161，效果更好，进行保存\n",
      "Epoch [61/2000], Loss: 1.1756\n",
      "Loss: 1.1756，效果更好，进行保存\n",
      "Epoch [62/2000], Loss: 1.1355\n",
      "Loss: 1.1355，效果更好，进行保存\n",
      "Epoch [63/2000], Loss: 1.1072\n",
      "Loss: 1.1072，效果更好，进行保存\n",
      "Epoch [64/2000], Loss: 1.0695\n",
      "Loss: 1.0695，效果更好，进行保存\n",
      "Epoch [65/2000], Loss: 1.0325\n",
      "Loss: 1.0325，效果更好，进行保存\n",
      "Epoch [66/2000], Loss: 1.0094\n",
      "Loss: 1.0094，效果更好，进行保存\n",
      "Epoch [67/2000], Loss: 0.9859\n",
      "Loss: 0.9859，效果更好，进行保存\n",
      "Epoch [68/2000], Loss: 0.9675\n",
      "Loss: 0.9675，效果更好，进行保存\n",
      "Epoch [69/2000], Loss: 0.9503\n",
      "Loss: 0.9503，效果更好，进行保存\n",
      "Epoch [70/2000], Loss: 0.9276\n",
      "Loss: 0.9276，效果更好，进行保存\n",
      "Epoch [71/2000], Loss: 0.9021\n",
      "Loss: 0.9021，效果更好，进行保存\n",
      "Epoch [72/2000], Loss: 0.8902\n",
      "Loss: 0.8902，效果更好，进行保存\n",
      "Epoch [73/2000], Loss: 0.8647\n",
      "Loss: 0.8647，效果更好，进行保存\n",
      "Epoch [74/2000], Loss: 0.8506\n",
      "Loss: 0.8506，效果更好，进行保存\n",
      "Epoch [75/2000], Loss: 0.8332\n",
      "Loss: 0.8332，效果更好，进行保存\n",
      "Epoch [76/2000], Loss: 0.8223\n",
      "Loss: 0.8223，效果更好，进行保存\n",
      "Epoch [77/2000], Loss: 0.8167\n",
      "Loss: 0.8167，效果更好，进行保存\n",
      "Epoch [78/2000], Loss: 0.7955\n",
      "Loss: 0.7955，效果更好，进行保存\n",
      "Epoch [79/2000], Loss: 0.7613\n",
      "Loss: 0.7613，效果更好，进行保存\n",
      "Epoch [80/2000], Loss: 0.7360\n",
      "Loss: 0.7360，效果更好，进行保存\n",
      "Epoch [81/2000], Loss: 0.7094\n",
      "Loss: 0.7094，效果更好，进行保存\n",
      "Epoch [82/2000], Loss: 0.6855\n",
      "Loss: 0.6855，效果更好，进行保存\n",
      "Epoch [83/2000], Loss: 0.6651\n",
      "Loss: 0.6651，效果更好，进行保存\n",
      "Epoch [84/2000], Loss: 0.6520\n",
      "Loss: 0.6520，效果更好，进行保存\n",
      "Epoch [85/2000], Loss: 0.6345\n",
      "Loss: 0.6345，效果更好，进行保存\n",
      "Epoch [86/2000], Loss: 0.6180\n",
      "Loss: 0.6180，效果更好，进行保存\n",
      "Epoch [87/2000], Loss: 0.6028\n",
      "Loss: 0.6028，效果更好，进行保存\n",
      "Epoch [88/2000], Loss: 0.5907\n",
      "Loss: 0.5907，效果更好，进行保存\n",
      "Epoch [89/2000], Loss: 0.5767\n",
      "Loss: 0.5767，效果更好，进行保存\n",
      "Epoch [90/2000], Loss: 0.5719\n",
      "Loss: 0.5719，效果更好，进行保存\n",
      "Epoch [91/2000], Loss: 0.5610\n",
      "Loss: 0.5610，效果更好，进行保存\n",
      "Epoch [92/2000], Loss: 0.5574\n",
      "Loss: 0.5574，效果更好，进行保存\n",
      "Epoch [93/2000], Loss: 0.5411\n",
      "Loss: 0.5411，效果更好，进行保存\n",
      "Epoch [94/2000], Loss: 0.5283\n",
      "Loss: 0.5283，效果更好，进行保存\n",
      "Epoch [95/2000], Loss: 0.5162\n",
      "Loss: 0.5162，效果更好，进行保存\n",
      "Epoch [96/2000], Loss: 0.4942\n",
      "Loss: 0.4942，效果更好，进行保存\n",
      "Epoch [97/2000], Loss: 0.4759\n",
      "Loss: 0.4759，效果更好，进行保存\n",
      "Epoch [98/2000], Loss: 0.4679\n",
      "Loss: 0.4679，效果更好，进行保存\n",
      "Epoch [99/2000], Loss: 0.4475\n",
      "Loss: 0.4475，效果更好，进行保存\n",
      "Epoch [100/2000], Loss: 0.4298\n",
      "Loss: 0.4298，效果更好，进行保存\n",
      "Epoch [101/2000], Loss: 0.4143\n",
      "Loss: 0.4143，效果更好，进行保存\n",
      "Epoch [102/2000], Loss: 0.4030\n",
      "Loss: 0.4030，效果更好，进行保存\n",
      "Epoch [103/2000], Loss: 0.3956\n",
      "Loss: 0.3956，效果更好，进行保存\n",
      "Epoch [104/2000], Loss: 0.3872\n",
      "Loss: 0.3872，效果更好，进行保存\n",
      "Epoch [105/2000], Loss: 0.3838\n",
      "Loss: 0.3838，效果更好，进行保存\n",
      "Epoch [106/2000], Loss: 0.3936\n",
      "Epoch [107/2000], Loss: 0.3993\n",
      "Epoch [108/2000], Loss: 0.3966\n",
      "Epoch [109/2000], Loss: 0.4079\n",
      "Epoch [110/2000], Loss: 0.3925\n",
      "Epoch [111/2000], Loss: 0.3679\n",
      "Loss: 0.3679，效果更好，进行保存\n",
      "Epoch [112/2000], Loss: 0.3409\n",
      "Loss: 0.3409，效果更好，进行保存\n",
      "Epoch [113/2000], Loss: 0.3233\n",
      "Loss: 0.3233，效果更好，进行保存\n",
      "Epoch [114/2000], Loss: 0.3115\n",
      "Loss: 0.3115，效果更好，进行保存\n",
      "Epoch [115/2000], Loss: 0.2978\n",
      "Loss: 0.2978，效果更好，进行保存\n",
      "Epoch [116/2000], Loss: 0.2871\n",
      "Loss: 0.2871，效果更好，进行保存\n",
      "Epoch [117/2000], Loss: 0.2778\n",
      "Loss: 0.2778，效果更好，进行保存\n",
      "Epoch [118/2000], Loss: 0.2702\n",
      "Loss: 0.2702，效果更好，进行保存\n",
      "Epoch [119/2000], Loss: 0.2620\n",
      "Loss: 0.2620，效果更好，进行保存\n",
      "Epoch [120/2000], Loss: 0.2570\n",
      "Loss: 0.2570，效果更好，进行保存\n",
      "Epoch [1031/2000], Loss: 0.0013\n",
      "Epoch [1032/2000], Loss: 0.0014\n",
      "Epoch [1033/2000], Loss: 0.0014\n",
      "Epoch [1034/2000], Loss: 0.0013\n",
      "Epoch [1035/2000], Loss: 0.0015\n",
      "Epoch [1036/2000], Loss: 0.0014\n",
      "Epoch [1037/2000], Loss: 0.0015\n",
      "Epoch [1038/2000], Loss: 0.0014\n",
      "Epoch [1039/2000], Loss: 0.0014\n",
      "Epoch [1040/2000], Loss: 0.0014\n",
      "Epoch [1041/2000], Loss: 0.0015\n",
      "Epoch [1042/2000], Loss: 0.0014\n",
      "Epoch [1043/2000], Loss: 0.0015\n",
      "Epoch [1044/2000], Loss: 0.0013\n",
      "Epoch [1045/2000], Loss: 0.0014\n",
      "Epoch [1046/2000], Loss: 0.0015\n",
      "Epoch [1047/2000], Loss: 0.0014\n",
      "Epoch [1048/2000], Loss: 0.0014\n",
      "Epoch [1049/2000], Loss: 0.0014\n",
      "Epoch [1050/2000], Loss: 0.0015\n",
      "Epoch [1051/2000], Loss: 0.0015\n",
      "Epoch [1052/2000], Loss: 0.0014\n",
      "Epoch [1053/2000], Loss: 0.0014\n",
      "Epoch [1054/2000], Loss: 0.0012\n",
      "Epoch [1055/2000], Loss: 0.0014\n",
      "Epoch [1056/2000], Loss: 0.0015\n",
      "Epoch [1057/2000], Loss: 0.0016\n",
      "Epoch [1058/2000], Loss: 0.0013\n",
      "Epoch [1059/2000], Loss: 0.0016\n",
      "Epoch [1060/2000], Loss: 0.0013\n",
      "Epoch [1061/2000], Loss: 0.0014\n",
      "Epoch [1062/2000], Loss: 0.0012\n",
      "Epoch [1063/2000], Loss: 0.0014\n",
      "Epoch [1064/2000], Loss: 0.0013\n",
      "Epoch [1065/2000], Loss: 0.0014\n",
      "Epoch [1066/2000], Loss: 0.0013\n",
      "Epoch [1067/2000], Loss: 0.0014\n",
      "Epoch [1068/2000], Loss: 0.0013\n",
      "Epoch [1069/2000], Loss: 0.0013\n",
      "Epoch [1070/2000], Loss: 0.0014\n",
      "Epoch [1071/2000], Loss: 0.0013\n",
      "Epoch [1072/2000], Loss: 0.0012\n",
      "Epoch [1073/2000], Loss: 0.0013\n",
      "Epoch [1074/2000], Loss: 0.0014\n",
      "Epoch [1075/2000], Loss: 0.0012\n",
      "Epoch [1076/2000], Loss: 0.0014\n",
      "Epoch [1077/2000], Loss: 0.0013\n",
      "Epoch [1078/2000], Loss: 0.0014\n",
      "Epoch [1079/2000], Loss: 0.0013\n",
      "Epoch [1080/2000], Loss: 0.0014\n",
      "Epoch [1081/2000], Loss: 0.0014\n",
      "Epoch [1082/2000], Loss: 0.0013\n",
      "Epoch [1083/2000], Loss: 0.0011\n",
      "Epoch [1084/2000], Loss: 0.0012\n",
      "Epoch [1085/2000], Loss: 0.0012\n",
      "Epoch [1086/2000], Loss: 0.0013\n",
      "Epoch [1087/2000], Loss: 0.0013\n",
      "Epoch [1088/2000], Loss: 0.0012\n",
      "Epoch [1089/2000], Loss: 0.0012\n",
      "Epoch [1090/2000], Loss: 0.0013\n",
      "Epoch [1091/2000], Loss: 0.0013\n",
      "Epoch [1092/2000], Loss: 0.0013\n",
      "Epoch [1093/2000], Loss: 0.0013\n",
      "Epoch [1094/2000], Loss: 0.0013\n",
      "Epoch [1095/2000], Loss: 0.0012\n",
      "Epoch [1096/2000], Loss: 0.0012\n",
      "Epoch [1097/2000], Loss: 0.0012\n",
      "Epoch [1098/2000], Loss: 0.0013\n",
      "Epoch [1099/2000], Loss: 0.0012\n",
      "Epoch [1100/2000], Loss: 0.0012\n",
      "Epoch [1101/2000], Loss: 0.0013\n",
      "Epoch [1102/2000], Loss: 0.0012\n",
      "Epoch [1103/2000], Loss: 0.0012\n",
      "Epoch [1104/2000], Loss: 0.0012\n",
      "Epoch [1105/2000], Loss: 0.0013\n",
      "Epoch [1106/2000], Loss: 0.0013\n",
      "Epoch [1107/2000], Loss: 0.0013\n",
      "Epoch [1108/2000], Loss: 0.0011\n",
      "Epoch [1109/2000], Loss: 0.0012\n",
      "Epoch [1110/2000], Loss: 0.0013\n",
      "Epoch [1111/2000], Loss: 0.0012\n",
      "Epoch [1112/2000], Loss: 0.0013\n",
      "Epoch [1113/2000], Loss: 0.0013\n",
      "Epoch [1114/2000], Loss: 0.0012\n",
      "Epoch [1115/2000], Loss: 0.0011\n",
      "Epoch [1116/2000], Loss: 0.0010\n",
      "Epoch [1117/2000], Loss: 0.0012\n",
      "Epoch [1118/2000], Loss: 0.0013\n",
      "Epoch [1119/2000], Loss: 0.0012\n",
      "Epoch [1120/2000], Loss: 0.0012\n",
      "Epoch [1121/2000], Loss: 0.0014\n",
      "Epoch [1122/2000], Loss: 0.0013\n",
      "Epoch [1123/2000], Loss: 0.0012\n",
      "Epoch [1124/2000], Loss: 0.0012\n",
      "Epoch [1125/2000], Loss: 0.0012\n",
      "Epoch [1126/2000], Loss: 0.0010\n",
      "Epoch [1127/2000], Loss: 0.0012\n",
      "Epoch [1128/2000], Loss: 0.0012\n",
      "Epoch [1129/2000], Loss: 0.0011\n",
      "Epoch [1130/2000], Loss: 0.0011\n",
      "Epoch [1131/2000], Loss: 0.0012\n",
      "Epoch [1132/2000], Loss: 0.0011\n",
      "Epoch [1133/2000], Loss: 0.0011\n",
      "Epoch [1134/2000], Loss: 0.0011\n",
      "Epoch [1135/2000], Loss: 0.0012\n",
      "Epoch [1136/2000], Loss: 0.0012\n",
      "Epoch [1137/2000], Loss: 0.0011\n",
      "Epoch [1138/2000], Loss: 0.0013\n",
      "Epoch [1139/2000], Loss: 0.0012\n",
      "Epoch [1140/2000], Loss: 0.0013\n",
      "Epoch [1141/2000], Loss: 0.0011\n",
      "Epoch [1142/2000], Loss: 0.0011\n",
      "Epoch [1143/2000], Loss: 0.0012\n",
      "Epoch [1144/2000], Loss: 0.0013\n",
      "Epoch [1145/2000], Loss: 0.0012\n",
      "Epoch [1146/2000], Loss: 0.0011\n",
      "Epoch [1147/2000], Loss: 0.0011\n",
      "Epoch [1148/2000], Loss: 0.0012\n",
      "Epoch [1149/2000], Loss: 0.0013\n",
      "Epoch [1150/2000], Loss: 0.0012\n",
      "Epoch [1151/2000], Loss: 0.0012\n",
      "Epoch [1152/2000], Loss: 0.0011\n",
      "Epoch [1153/2000], Loss: 0.0012\n",
      "Epoch [1154/2000], Loss: 0.0013\n",
      "Epoch [1155/2000], Loss: 0.0011\n",
      "Epoch [1156/2000], Loss: 0.0012\n",
      "Epoch [1157/2000], Loss: 0.0012\n",
      "Epoch [1158/2000], Loss: 0.0012\n",
      "Epoch [1159/2000], Loss: 0.0013\n",
      "Epoch [1160/2000], Loss: 0.0011\n",
      "Epoch [1161/2000], Loss: 0.0010\n",
      "Epoch [1162/2000], Loss: 0.0011\n",
      "Epoch [1163/2000], Loss: 0.0012\n",
      "Epoch [1164/2000], Loss: 0.0011\n",
      "Epoch [1165/2000], Loss: 0.0011\n",
      "Epoch [1166/2000], Loss: 0.0012\n",
      "Epoch [1167/2000], Loss: 0.0012\n",
      "Epoch [1168/2000], Loss: 0.0011\n",
      "Epoch [1169/2000], Loss: 0.0010\n",
      "Epoch [1170/2000], Loss: 0.0012\n",
      "Epoch [1171/2000], Loss: 0.0011\n",
      "Epoch [1172/2000], Loss: 0.0012\n",
      "Epoch [1173/2000], Loss: 0.0012\n",
      "Epoch [1174/2000], Loss: 0.0012\n",
      "Epoch [1175/2000], Loss: 0.0011\n",
      "Epoch [1176/2000], Loss: 0.0011\n",
      "Epoch [1177/2000], Loss: 0.0012\n",
      "Epoch [1178/2000], Loss: 0.0010\n",
      "Epoch [1179/2000], Loss: 0.0012\n",
      "Epoch [1180/2000], Loss: 0.0012\n",
      "Epoch [1181/2000], Loss: 0.0010\n",
      "Epoch [1182/2000], Loss: 0.0013\n",
      "Epoch [1183/2000], Loss: 0.0011\n",
      "Epoch [1184/2000], Loss: 0.0011\n",
      "Epoch [1185/2000], Loss: 0.0012\n",
      "Epoch [1186/2000], Loss: 0.0011\n",
      "Epoch [1187/2000], Loss: 0.0012\n",
      "Epoch [1188/2000], Loss: 0.0010\n",
      "Epoch [1189/2000], Loss: 0.0011\n",
      "Epoch [1190/2000], Loss: 0.0011\n",
      "Epoch [1191/2000], Loss: 0.0012\n",
      "Epoch [1192/2000], Loss: 0.0011\n",
      "Epoch [1193/2000], Loss: 0.0011\n",
      "Epoch [1194/2000], Loss: 0.0011\n",
      "Epoch [1195/2000], Loss: 0.0011\n",
      "Epoch [1196/2000], Loss: 0.0012\n",
      "Epoch [1197/2000], Loss: 0.0010\n",
      "Epoch [1198/2000], Loss: 0.0012\n",
      "Epoch [1199/2000], Loss: 0.0010\n",
      "Epoch [1200/2000], Loss: 0.0010\n",
      "Epoch [1201/2000], Loss: 0.0011\n",
      "Epoch [1202/2000], Loss: 0.0010\n",
      "Epoch [1203/2000], Loss: 0.0010\n",
      "Epoch [1204/2000], Loss: 0.0012\n",
      "Epoch [1205/2000], Loss: 0.0011\n",
      "Epoch [1206/2000], Loss: 0.0010\n",
      "Epoch [1207/2000], Loss: 0.0011\n",
      "Epoch [1208/2000], Loss: 0.0012\n",
      "Epoch [1209/2000], Loss: 0.0011\n",
      "Epoch [1210/2000], Loss: 0.0011\n",
      "Epoch [1211/2000], Loss: 0.0014\n",
      "Epoch [1212/2000], Loss: 0.0010\n",
      "Epoch [1213/2000], Loss: 0.0010\n",
      "Epoch [1214/2000], Loss: 0.0010\n",
      "Epoch [1215/2000], Loss: 0.0011\n",
      "Epoch [1216/2000], Loss: 0.0010\n",
      "Epoch [1217/2000], Loss: 0.0010\n",
      "Epoch [1218/2000], Loss: 0.0011\n",
      "Epoch [1219/2000], Loss: 0.0010\n",
      "Epoch [1220/2000], Loss: 0.0010\n",
      "Epoch [1221/2000], Loss: 0.0011\n",
      "Epoch [1222/2000], Loss: 0.0012\n",
      "Epoch [1223/2000], Loss: 0.0011\n",
      "Epoch [1224/2000], Loss: 0.0011\n",
      "Epoch [1225/2000], Loss: 0.0010\n",
      "Epoch [1226/2000], Loss: 0.0010\n",
      "Epoch [1227/2000], Loss: 0.0010\n",
      "Epoch [1228/2000], Loss: 0.0011\n",
      "Epoch [1229/2000], Loss: 0.0012\n",
      "Epoch [1230/2000], Loss: 0.0012\n",
      "Epoch [1231/2000], Loss: 0.0012\n",
      "Epoch [1232/2000], Loss: 0.0011\n",
      "Epoch [1233/2000], Loss: 0.0010\n",
      "Epoch [1234/2000], Loss: 0.0012\n",
      "Epoch [1235/2000], Loss: 0.0012\n",
      "Epoch [1236/2000], Loss: 0.0012\n",
      "Epoch [1237/2000], Loss: 0.0010\n",
      "Epoch [1238/2000], Loss: 0.0010\n",
      "Epoch [1239/2000], Loss: 0.0011\n",
      "Epoch [1240/2000], Loss: 0.0011\n",
      "Epoch [1241/2000], Loss: 0.0010\n",
      "Epoch [1242/2000], Loss: 0.0011\n",
      "Epoch [1243/2000], Loss: 0.0011\n",
      "Epoch [1244/2000], Loss: 0.0009\n",
      "Epoch [1245/2000], Loss: 0.0009\n",
      "Epoch [1246/2000], Loss: 0.0011\n",
      "Epoch [1247/2000], Loss: 0.0012\n",
      "Epoch [1248/2000], Loss: 0.0010\n",
      "Epoch [1249/2000], Loss: 0.0010\n",
      "Epoch [1250/2000], Loss: 0.0011\n",
      "Epoch [1251/2000], Loss: 0.0009\n",
      "Epoch [1252/2000], Loss: 0.0012\n",
      "Epoch [1253/2000], Loss: 0.0012\n",
      "Epoch [1254/2000], Loss: 0.0011\n",
      "Epoch [1255/2000], Loss: 0.0013\n",
      "Epoch [1256/2000], Loss: 0.0012\n",
      "Epoch [1257/2000], Loss: 0.0011\n",
      "Epoch [1258/2000], Loss: 0.0012\n",
      "Epoch [1259/2000], Loss: 0.0010\n",
      "Epoch [1260/2000], Loss: 0.0011\n",
      "Epoch [1261/2000], Loss: 0.0011\n",
      "Epoch [1262/2000], Loss: 0.0011\n",
      "Epoch [1263/2000], Loss: 0.0011\n",
      "Epoch [1264/2000], Loss: 0.0010\n",
      "Epoch [1265/2000], Loss: 0.0011\n",
      "Epoch [1266/2000], Loss: 0.0011\n",
      "Epoch [1267/2000], Loss: 0.0011\n",
      "Epoch [1268/2000], Loss: 0.0011\n",
      "Epoch [1269/2000], Loss: 0.0011\n",
      "Epoch [1270/2000], Loss: 0.0010\n",
      "Epoch [1271/2000], Loss: 0.0011\n",
      "Epoch [1272/2000], Loss: 0.0009\n",
      "Loss: 0.0009，效果更好，进行保存\n",
      "Epoch [1273/2000], Loss: 0.0010\n",
      "Epoch [1274/2000], Loss: 0.0011\n",
      "Epoch [1275/2000], Loss: 0.0009\n",
      "Epoch [1276/2000], Loss: 0.0010\n",
      "Epoch [1277/2000], Loss: 0.0009\n",
      "Epoch [1278/2000], Loss: 0.0010\n",
      "Epoch [1279/2000], Loss: 0.0010\n",
      "Epoch [1280/2000], Loss: 0.0009\n",
      "Epoch [1281/2000], Loss: 0.0010\n",
      "Epoch [1282/2000], Loss: 0.0010\n",
      "Epoch [1283/2000], Loss: 0.0012\n",
      "Epoch [1284/2000], Loss: 0.0010\n",
      "Epoch [1285/2000], Loss: 0.0010\n",
      "Epoch [1286/2000], Loss: 0.0011\n",
      "Epoch [1287/2000], Loss: 0.0011\n",
      "Epoch [1288/2000], Loss: 0.0009\n",
      "Epoch [1289/2000], Loss: 0.0011\n",
      "Epoch [1290/2000], Loss: 0.0011\n",
      "Epoch [1291/2000], Loss: 0.0009\n",
      "Epoch [1292/2000], Loss: 0.0011\n",
      "Epoch [1293/2000], Loss: 0.0012\n",
      "Epoch [1294/2000], Loss: 0.0010\n",
      "Epoch [1295/2000], Loss: 0.0010\n",
      "Epoch [1296/2000], Loss: 0.0010\n",
      "Epoch [1297/2000], Loss: 0.0010\n",
      "Epoch [1298/2000], Loss: 0.0010\n",
      "Epoch [1299/2000], Loss: 0.0010\n",
      "Epoch [1300/2000], Loss: 0.0012\n",
      "Epoch [1301/2000], Loss: 0.0012\n",
      "Epoch [1302/2000], Loss: 0.0010\n",
      "Epoch [1303/2000], Loss: 0.0011\n",
      "Epoch [1304/2000], Loss: 0.0011\n",
      "Epoch [1305/2000], Loss: 0.0011\n",
      "Epoch [1306/2000], Loss: 0.0010\n",
      "Epoch [1307/2000], Loss: 0.0011\n",
      "Epoch [1308/2000], Loss: 0.0011\n",
      "Epoch [1309/2000], Loss: 0.0010\n",
      "Epoch [1310/2000], Loss: 0.0011\n",
      "Epoch [1311/2000], Loss: 0.0012\n",
      "Epoch [1312/2000], Loss: 0.0010\n",
      "Epoch [1313/2000], Loss: 0.0011\n",
      "Epoch [1314/2000], Loss: 0.0011\n",
      "Epoch [1315/2000], Loss: 0.0010\n",
      "Epoch [1316/2000], Loss: 0.0011\n",
      "Epoch [1317/2000], Loss: 0.0010\n",
      "Epoch [1318/2000], Loss: 0.0009\n",
      "Epoch [1319/2000], Loss: 0.0009\n",
      "Epoch [1320/2000], Loss: 0.0010\n",
      "Epoch [1321/2000], Loss: 0.0011\n",
      "Epoch [1322/2000], Loss: 0.0009\n",
      "Epoch [1323/2000], Loss: 0.0011\n",
      "Epoch [1324/2000], Loss: 0.0009\n",
      "Loss: 0.0009，效果更好，进行保存\n",
      "Epoch [1325/2000], Loss: 0.0010\n",
      "Epoch [1326/2000], Loss: 0.0010\n",
      "Epoch [1327/2000], Loss: 0.0010\n",
      "Epoch [1328/2000], Loss: 0.0009\n",
      "Epoch [1329/2000], Loss: 0.0010\n",
      "Epoch [1330/2000], Loss: 0.0010\n",
      "Epoch [1331/2000], Loss: 0.0009\n",
      "Epoch [1332/2000], Loss: 0.0010\n",
      "Epoch [1333/2000], Loss: 0.0011\n",
      "Epoch [1334/2000], Loss: 0.0011\n",
      "Epoch [1335/2000], Loss: 0.0011\n",
      "Epoch [1336/2000], Loss: 0.0010\n",
      "Epoch [1337/2000], Loss: 0.0009\n",
      "Epoch [1338/2000], Loss: 0.0011\n",
      "Epoch [1339/2000], Loss: 0.0010\n",
      "Epoch [1340/2000], Loss: 0.0011\n",
      "Epoch [1341/2000], Loss: 0.0012\n",
      "Epoch [1342/2000], Loss: 0.0009\n",
      "Epoch [1343/2000], Loss: 0.0010\n",
      "Epoch [1344/2000], Loss: 0.0012\n",
      "Epoch [1345/2000], Loss: 0.0010\n",
      "Epoch [1346/2000], Loss: 0.0010\n",
      "Epoch [1347/2000], Loss: 0.0010\n",
      "Epoch [1348/2000], Loss: 0.0011\n",
      "Epoch [1349/2000], Loss: 0.0010\n",
      "Epoch [1350/2000], Loss: 0.0010\n",
      "Epoch [1351/2000], Loss: 0.0012\n",
      "Epoch [1352/2000], Loss: 0.0010\n",
      "Epoch [1353/2000], Loss: 0.0011\n",
      "Epoch [1354/2000], Loss: 0.0011\n",
      "Epoch [1355/2000], Loss: 0.0010\n",
      "Epoch [1356/2000], Loss: 0.0009\n",
      "Epoch [1357/2000], Loss: 0.0008\n",
      "Loss: 0.0008，效果更好，进行保存\n",
      "Epoch [1358/2000], Loss: 0.0011\n",
      "Epoch [1359/2000], Loss: 0.0009\n",
      "Epoch [1360/2000], Loss: 0.0009\n",
      "Epoch [1361/2000], Loss: 0.0010\n",
      "Epoch [1362/2000], Loss: 0.0011\n",
      "Epoch [1363/2000], Loss: 0.0010\n",
      "Epoch [1364/2000], Loss: 0.0010\n",
      "Epoch [1365/2000], Loss: 0.0011\n",
      "Epoch [1366/2000], Loss: 0.0010\n",
      "Epoch [1367/2000], Loss: 0.0010\n",
      "Epoch [1368/2000], Loss: 0.0010\n",
      "Epoch [1369/2000], Loss: 0.0011\n",
      "Epoch [1370/2000], Loss: 0.0011\n",
      "Epoch [1371/2000], Loss: 0.0011\n",
      "Epoch [1372/2000], Loss: 0.0011\n",
      "Epoch [1373/2000], Loss: 0.0012\n",
      "Epoch [1374/2000], Loss: 0.0010\n",
      "Epoch [1375/2000], Loss: 0.0009\n",
      "Epoch [1376/2000], Loss: 0.0010\n",
      "Epoch [1377/2000], Loss: 0.0010\n",
      "Epoch [1378/2000], Loss: 0.0011\n",
      "Epoch [1379/2000], Loss: 0.0010\n",
      "Epoch [1380/2000], Loss: 0.0011\n",
      "Epoch [1381/2000], Loss: 0.0010\n",
      "Epoch [1382/2000], Loss: 0.0009\n",
      "Epoch [1383/2000], Loss: 0.0009\n",
      "Epoch [1384/2000], Loss: 0.0011\n",
      "Epoch [1385/2000], Loss: 0.0010\n",
      "Epoch [1386/2000], Loss: 0.0010\n",
      "Epoch [1387/2000], Loss: 0.0010\n",
      "Epoch [1388/2000], Loss: 0.0010\n",
      "Epoch [1389/2000], Loss: 0.0010\n",
      "Epoch [1390/2000], Loss: 0.0009\n",
      "Epoch [1391/2000], Loss: 0.0010\n",
      "Epoch [1392/2000], Loss: 0.0010\n",
      "Epoch [1393/2000], Loss: 0.0010\n",
      "Epoch [1394/2000], Loss: 0.0010\n",
      "Epoch [1395/2000], Loss: 0.0009\n",
      "Epoch [1396/2000], Loss: 0.0010\n",
      "Epoch [1397/2000], Loss: 0.0011\n",
      "Epoch [1398/2000], Loss: 0.0010\n",
      "Epoch [1399/2000], Loss: 0.0011\n",
      "Epoch [1400/2000], Loss: 0.0009\n",
      "Epoch [1401/2000], Loss: 0.0010\n",
      "Epoch [1402/2000], Loss: 0.0011\n",
      "Epoch [1403/2000], Loss: 0.0011\n",
      "Epoch [1404/2000], Loss: 0.0011\n",
      "Epoch [1405/2000], Loss: 0.0010\n",
      "Epoch [1406/2000], Loss: 0.0009\n",
      "Epoch [1407/2000], Loss: 0.0010\n",
      "Epoch [1408/2000], Loss: 0.0011\n",
      "Epoch [1409/2000], Loss: 0.0011\n",
      "Epoch [1410/2000], Loss: 0.0011\n",
      "Epoch [1411/2000], Loss: 0.0011\n",
      "Epoch [1412/2000], Loss: 0.0008\n",
      "Loss: 0.0008，效果更好，进行保存\n",
      "Epoch [1413/2000], Loss: 0.0011\n",
      "Epoch [1414/2000], Loss: 0.0010\n",
      "Epoch [1415/2000], Loss: 0.0011\n",
      "Epoch [1416/2000], Loss: 0.0011\n",
      "Epoch [1417/2000], Loss: 0.0008\n",
      "Epoch [1418/2000], Loss: 0.0011\n",
      "Epoch [1419/2000], Loss: 0.0010\n",
      "Epoch [1420/2000], Loss: 0.0010\n",
      "Epoch [1421/2000], Loss: 0.0011\n",
      "Epoch [1422/2000], Loss: 0.0012\n",
      "Epoch [1423/2000], Loss: 0.0010\n",
      "Epoch [1424/2000], Loss: 0.0011\n",
      "Epoch [1425/2000], Loss: 0.0010\n",
      "Epoch [1426/2000], Loss: 0.0010\n",
      "Epoch [1427/2000], Loss: 0.0011\n",
      "Epoch [1428/2000], Loss: 0.0011\n",
      "Epoch [1429/2000], Loss: 0.0011\n",
      "Epoch [1430/2000], Loss: 0.0010\n",
      "Epoch [1431/2000], Loss: 0.0011\n",
      "Epoch [1432/2000], Loss: 0.0010\n",
      "Epoch [1433/2000], Loss: 0.0011\n",
      "Epoch [1434/2000], Loss: 0.0010\n",
      "Epoch [1435/2000], Loss: 0.0010\n",
      "Epoch [1436/2000], Loss: 0.0009\n",
      "Epoch [1437/2000], Loss: 0.0010\n",
      "Epoch [1438/2000], Loss: 0.0011\n",
      "Epoch [1439/2000], Loss: 0.0011\n",
      "Epoch [1440/2000], Loss: 0.0010\n",
      "Epoch [1441/2000], Loss: 0.0010\n",
      "Epoch [1442/2000], Loss: 0.0009\n",
      "Epoch [1443/2000], Loss: 0.0009\n",
      "Epoch [1444/2000], Loss: 0.0011\n",
      "Epoch [1445/2000], Loss: 0.0010\n",
      "Epoch [1446/2000], Loss: 0.0011\n",
      "Epoch [1447/2000], Loss: 0.0012\n",
      "Epoch [1448/2000], Loss: 0.0011\n",
      "Epoch [1449/2000], Loss: 0.0011\n",
      "Epoch [1450/2000], Loss: 0.0012\n",
      "Epoch [1451/2000], Loss: 0.0021\n",
      "Epoch [1452/2000], Loss: 0.0047\n",
      "Epoch [1453/2000], Loss: 0.0161\n",
      "Epoch [1454/2000], Loss: 0.0481\n",
      "Epoch [1455/2000], Loss: 0.1551\n",
      "Epoch [1456/2000], Loss: 0.2488\n",
      "Epoch [1457/2000], Loss: 0.2267\n",
      "Epoch [1458/2000], Loss: 0.1566\n",
      "Epoch [1459/2000], Loss: 0.0979\n",
      "Epoch [1460/2000], Loss: 0.0485\n",
      "Epoch [1461/2000], Loss: 0.0168\n",
      "Epoch [1462/2000], Loss: 0.0079\n",
      "Epoch [1463/2000], Loss: 0.0044\n",
      "Epoch [1464/2000], Loss: 0.0030\n",
      "Epoch [1465/2000], Loss: 0.0026\n",
      "Epoch [1466/2000], Loss: 0.0024\n",
      "Epoch [1467/2000], Loss: 0.0023\n",
      "Epoch [1468/2000], Loss: 0.0021\n",
      "Epoch [1469/2000], Loss: 0.0020\n",
      "Epoch [1470/2000], Loss: 0.0020\n",
      "Epoch [1471/2000], Loss: 0.0019\n",
      "Epoch [1472/2000], Loss: 0.0019\n",
      "Epoch [1473/2000], Loss: 0.0018\n",
      "Epoch [1474/2000], Loss: 0.0019\n",
      "Epoch [1475/2000], Loss: 0.0017\n",
      "Epoch [1476/2000], Loss: 0.0017\n",
      "Epoch [1477/2000], Loss: 0.0017\n",
      "Epoch [1478/2000], Loss: 0.0017\n",
      "Epoch [1479/2000], Loss: 0.0015\n",
      "Epoch [1480/2000], Loss: 0.0016\n",
      "Epoch [1481/2000], Loss: 0.0016\n",
      "Epoch [1482/2000], Loss: 0.0017\n",
      "Epoch [1483/2000], Loss: 0.0015\n",
      "Epoch [1484/2000], Loss: 0.0016\n",
      "Epoch [1485/2000], Loss: 0.0016\n",
      "Epoch [1486/2000], Loss: 0.0015\n",
      "Epoch [1487/2000], Loss: 0.0015\n",
      "Epoch [1488/2000], Loss: 0.0015\n",
      "Epoch [1489/2000], Loss: 0.0015\n",
      "Epoch [1490/2000], Loss: 0.0015\n",
      "Epoch [1491/2000], Loss: 0.0015\n",
      "Epoch [1492/2000], Loss: 0.0015\n",
      "Epoch [1493/2000], Loss: 0.0016\n",
      "Epoch [1494/2000], Loss: 0.0014\n",
      "Epoch [1495/2000], Loss: 0.0014\n",
      "Epoch [1496/2000], Loss: 0.0015\n",
      "Epoch [1497/2000], Loss: 0.0014\n",
      "Epoch [1498/2000], Loss: 0.0014\n",
      "Epoch [1499/2000], Loss: 0.0013\n",
      "Epoch [1500/2000], Loss: 0.0012\n",
      "Epoch [1501/2000], Loss: 0.0014\n",
      "Epoch [1502/2000], Loss: 0.0014\n",
      "Epoch [1503/2000], Loss: 0.0013\n",
      "Epoch [1504/2000], Loss: 0.0013\n",
      "Epoch [1505/2000], Loss: 0.0013\n",
      "Epoch [1506/2000], Loss: 0.0013\n",
      "Epoch [1507/2000], Loss: 0.0014\n",
      "Epoch [1508/2000], Loss: 0.0013\n",
      "Epoch [1509/2000], Loss: 0.0014\n",
      "Epoch [1510/2000], Loss: 0.0014\n",
      "Epoch [1511/2000], Loss: 0.0013\n",
      "Epoch [1512/2000], Loss: 0.0014\n",
      "Epoch [1513/2000], Loss: 0.0012\n",
      "Epoch [1514/2000], Loss: 0.0015\n",
      "Epoch [1515/2000], Loss: 0.0013\n",
      "Epoch [1516/2000], Loss: 0.0014\n",
      "Epoch [1517/2000], Loss: 0.0013\n",
      "Epoch [1518/2000], Loss: 0.0012\n",
      "Epoch [1519/2000], Loss: 0.0012\n",
      "Epoch [1520/2000], Loss: 0.0013\n",
      "Epoch [1521/2000], Loss: 0.0012\n",
      "Epoch [1522/2000], Loss: 0.0012\n",
      "Epoch [1523/2000], Loss: 0.0013\n",
      "Epoch [1524/2000], Loss: 0.0013\n",
      "Epoch [1525/2000], Loss: 0.0011\n",
      "Epoch [1526/2000], Loss: 0.0012\n",
      "Epoch [1527/2000], Loss: 0.0013\n",
      "Epoch [1528/2000], Loss: 0.0014\n",
      "Epoch [1529/2000], Loss: 0.0012\n",
      "Epoch [1530/2000], Loss: 0.0013\n",
      "Epoch [1531/2000], Loss: 0.0012\n",
      "Epoch [1532/2000], Loss: 0.0012\n",
      "Epoch [1533/2000], Loss: 0.0011\n",
      "Epoch [1534/2000], Loss: 0.0012\n",
      "Epoch [1535/2000], Loss: 0.0013\n",
      "Epoch [1536/2000], Loss: 0.0013\n",
      "Epoch [1537/2000], Loss: 0.0014\n",
      "Epoch [1538/2000], Loss: 0.0012\n",
      "Epoch [1539/2000], Loss: 0.0013\n",
      "Epoch [1540/2000], Loss: 0.0011\n",
      "Epoch [1541/2000], Loss: 0.0012\n",
      "Epoch [1542/2000], Loss: 0.0012\n",
      "Epoch [1543/2000], Loss: 0.0012\n",
      "Epoch [1544/2000], Loss: 0.0012\n",
      "Epoch [1545/2000], Loss: 0.0012\n",
      "Epoch [1546/2000], Loss: 0.0012\n",
      "Epoch [1547/2000], Loss: 0.0012\n",
      "Epoch [1548/2000], Loss: 0.0012\n",
      "Epoch [1549/2000], Loss: 0.0013\n",
      "Epoch [1550/2000], Loss: 0.0013\n",
      "Epoch [1551/2000], Loss: 0.0012\n",
      "Epoch [1552/2000], Loss: 0.0012\n",
      "Epoch [1553/2000], Loss: 0.0013\n",
      "Epoch [1554/2000], Loss: 0.0012\n",
      "Epoch [1555/2000], Loss: 0.0012\n",
      "Epoch [1556/2000], Loss: 0.0012\n",
      "Epoch [1557/2000], Loss: 0.0012\n",
      "Epoch [1558/2000], Loss: 0.0011\n",
      "Epoch [1559/2000], Loss: 0.0012\n",
      "Epoch [1560/2000], Loss: 0.0011\n",
      "Epoch [1561/2000], Loss: 0.0011\n",
      "Epoch [1562/2000], Loss: 0.0011\n",
      "Epoch [1563/2000], Loss: 0.0011\n",
      "Epoch [1564/2000], Loss: 0.0011\n",
      "Epoch [1565/2000], Loss: 0.0012\n",
      "Epoch [1566/2000], Loss: 0.0013\n",
      "Epoch [1567/2000], Loss: 0.0012\n",
      "Epoch [1568/2000], Loss: 0.0011\n",
      "Epoch [1569/2000], Loss: 0.0011\n",
      "Epoch [1570/2000], Loss: 0.0011\n",
      "Epoch [1571/2000], Loss: 0.0013\n",
      "Epoch [1572/2000], Loss: 0.0012\n",
      "Epoch [1573/2000], Loss: 0.0011\n",
      "Epoch [1574/2000], Loss: 0.0011\n",
      "Epoch [1575/2000], Loss: 0.0012\n",
      "Epoch [1576/2000], Loss: 0.0013\n",
      "Epoch [1577/2000], Loss: 0.0012\n",
      "Epoch [1578/2000], Loss: 0.0011\n",
      "Epoch [1579/2000], Loss: 0.0011\n",
      "Epoch [1580/2000], Loss: 0.0012\n",
      "Epoch [1581/2000], Loss: 0.0011\n",
      "Epoch [1582/2000], Loss: 0.0012\n",
      "Epoch [1583/2000], Loss: 0.0013\n",
      "Epoch [1584/2000], Loss: 0.0011\n",
      "Epoch [1585/2000], Loss: 0.0012\n",
      "Epoch [1586/2000], Loss: 0.0010\n",
      "Epoch [1587/2000], Loss: 0.0010\n",
      "Epoch [1588/2000], Loss: 0.0012\n",
      "Epoch [1589/2000], Loss: 0.0010\n",
      "Epoch [1590/2000], Loss: 0.0012\n",
      "Epoch [1591/2000], Loss: 0.0011\n",
      "Epoch [1592/2000], Loss: 0.0011\n",
      "Epoch [1593/2000], Loss: 0.0012\n",
      "Epoch [1594/2000], Loss: 0.0011\n",
      "Epoch [1595/2000], Loss: 0.0011\n",
      "Epoch [1596/2000], Loss: 0.0011\n",
      "Epoch [1597/2000], Loss: 0.0013\n",
      "Epoch [1598/2000], Loss: 0.0012\n",
      "Epoch [1599/2000], Loss: 0.0012\n",
      "Epoch [1600/2000], Loss: 0.0011\n",
      "Epoch [1601/2000], Loss: 0.0010\n",
      "Epoch [1602/2000], Loss: 0.0011\n",
      "Epoch [1603/2000], Loss: 0.0012\n",
      "Epoch [1604/2000], Loss: 0.0012\n",
      "Epoch [1605/2000], Loss: 0.0011\n",
      "Epoch [1606/2000], Loss: 0.0010\n",
      "Epoch [1607/2000], Loss: 0.0011\n",
      "Epoch [1608/2000], Loss: 0.0012\n",
      "Epoch [1609/2000], Loss: 0.0010\n",
      "Epoch [1610/2000], Loss: 0.0011\n",
      "Epoch [1611/2000], Loss: 0.0011\n",
      "Epoch [1612/2000], Loss: 0.0011\n",
      "Epoch [1613/2000], Loss: 0.0011\n",
      "Epoch [1614/2000], Loss: 0.0011\n",
      "Epoch [1615/2000], Loss: 0.0011\n",
      "Epoch [1616/2000], Loss: 0.0011\n",
      "Epoch [1617/2000], Loss: 0.0012\n",
      "Epoch [1618/2000], Loss: 0.0012\n",
      "Epoch [1619/2000], Loss: 0.0011\n",
      "Epoch [1620/2000], Loss: 0.0012\n",
      "Epoch [1621/2000], Loss: 0.0012\n",
      "Epoch [1622/2000], Loss: 0.0012\n",
      "Epoch [1623/2000], Loss: 0.0011\n",
      "Epoch [1624/2000], Loss: 0.0011\n",
      "Epoch [1625/2000], Loss: 0.0010\n",
      "Epoch [1626/2000], Loss: 0.0011\n",
      "Epoch [1627/2000], Loss: 0.0009\n",
      "Epoch [1628/2000], Loss: 0.0013\n",
      "Epoch [1629/2000], Loss: 0.0010\n",
      "Epoch [1630/2000], Loss: 0.0012\n",
      "Epoch [1631/2000], Loss: 0.0010\n",
      "Epoch [1632/2000], Loss: 0.0011\n",
      "Epoch [1633/2000], Loss: 0.0010\n",
      "Epoch [1634/2000], Loss: 0.0012\n",
      "Epoch [1635/2000], Loss: 0.0009\n",
      "Epoch [1636/2000], Loss: 0.0012\n",
      "Epoch [1637/2000], Loss: 0.0013\n",
      "Epoch [1638/2000], Loss: 0.0011\n",
      "Epoch [1639/2000], Loss: 0.0013\n",
      "Epoch [1640/2000], Loss: 0.0010\n",
      "Epoch [1641/2000], Loss: 0.0011\n",
      "Epoch [1642/2000], Loss: 0.0012\n",
      "Epoch [1643/2000], Loss: 0.0010\n",
      "Epoch [1644/2000], Loss: 0.0011\n",
      "Epoch [1645/2000], Loss: 0.0010\n",
      "Epoch [1646/2000], Loss: 0.0011\n",
      "Epoch [1647/2000], Loss: 0.0011\n",
      "Epoch [1648/2000], Loss: 0.0011\n",
      "Epoch [1649/2000], Loss: 0.0010\n",
      "Epoch [1650/2000], Loss: 0.0009\n",
      "Epoch [1651/2000], Loss: 0.0010\n",
      "Epoch [1652/2000], Loss: 0.0011\n",
      "Epoch [1653/2000], Loss: 0.0011\n",
      "Epoch [1654/2000], Loss: 0.0011\n",
      "Epoch [1655/2000], Loss: 0.0012\n",
      "Epoch [1656/2000], Loss: 0.0011\n",
      "Epoch [1657/2000], Loss: 0.0010\n",
      "Epoch [1658/2000], Loss: 0.0010\n",
      "Epoch [1659/2000], Loss: 0.0011\n",
      "Epoch [1660/2000], Loss: 0.0010\n",
      "Epoch [1661/2000], Loss: 0.0012\n",
      "Epoch [1662/2000], Loss: 0.0008\n",
      "Epoch [1663/2000], Loss: 0.0011\n",
      "Epoch [1664/2000], Loss: 0.0010\n",
      "Epoch [1665/2000], Loss: 0.0011\n",
      "Epoch [1666/2000], Loss: 0.0011\n",
      "Epoch [1667/2000], Loss: 0.0010\n",
      "Epoch [1668/2000], Loss: 0.0010\n",
      "Epoch [1669/2000], Loss: 0.0011\n",
      "Epoch [1670/2000], Loss: 0.0011\n",
      "Epoch [1671/2000], Loss: 0.0010\n",
      "Epoch [1672/2000], Loss: 0.0010\n",
      "Epoch [1673/2000], Loss: 0.0010\n",
      "Epoch [1674/2000], Loss: 0.0010\n",
      "Epoch [1675/2000], Loss: 0.0009\n",
      "Epoch [1676/2000], Loss: 0.0011\n",
      "Epoch [1677/2000], Loss: 0.0011\n",
      "Epoch [1678/2000], Loss: 0.0011\n",
      "Epoch [1679/2000], Loss: 0.0010\n",
      "Epoch [1680/2000], Loss: 0.0011\n",
      "Epoch [1681/2000], Loss: 0.0011\n",
      "Epoch [1682/2000], Loss: 0.0011\n",
      "Epoch [1683/2000], Loss: 0.0012\n",
      "Epoch [1684/2000], Loss: 0.0011\n",
      "Epoch [1685/2000], Loss: 0.0011\n",
      "Epoch [1686/2000], Loss: 0.0011\n",
      "Epoch [1687/2000], Loss: 0.0010\n",
      "Epoch [1688/2000], Loss: 0.0010\n",
      "Epoch [1689/2000], Loss: 0.0010\n",
      "Epoch [1690/2000], Loss: 0.0011\n",
      "Epoch [1691/2000], Loss: 0.0010\n",
      "Epoch [1692/2000], Loss: 0.0009\n",
      "Epoch [1693/2000], Loss: 0.0012\n",
      "Epoch [1694/2000], Loss: 0.0011\n",
      "Epoch [1695/2000], Loss: 0.0009\n",
      "Epoch [1696/2000], Loss: 0.0010\n",
      "Epoch [1697/2000], Loss: 0.0011\n",
      "Epoch [1698/2000], Loss: 0.0011\n",
      "Epoch [1699/2000], Loss: 0.0009\n",
      "Epoch [1700/2000], Loss: 0.0011\n",
      "Epoch [1701/2000], Loss: 0.0010\n",
      "Epoch [1702/2000], Loss: 0.0010\n",
      "Epoch [1703/2000], Loss: 0.0011\n",
      "Epoch [1704/2000], Loss: 0.0012\n",
      "Epoch [1705/2000], Loss: 0.0010\n",
      "Epoch [1706/2000], Loss: 0.0011\n",
      "Epoch [1707/2000], Loss: 0.0011\n",
      "Epoch [1708/2000], Loss: 0.0011\n",
      "Epoch [1709/2000], Loss: 0.0011\n",
      "Epoch [1710/2000], Loss: 0.0012\n",
      "Epoch [1711/2000], Loss: 0.0012\n",
      "Epoch [1712/2000], Loss: 0.0011\n",
      "Epoch [1713/2000], Loss: 0.0011\n",
      "Epoch [1714/2000], Loss: 0.0011\n",
      "Epoch [1715/2000], Loss: 0.0011\n",
      "Epoch [1716/2000], Loss: 0.0011\n",
      "Epoch [1717/2000], Loss: 0.0011\n",
      "Epoch [1718/2000], Loss: 0.0011\n",
      "Epoch [1719/2000], Loss: 0.0010\n",
      "Epoch [1720/2000], Loss: 0.0011\n",
      "Epoch [1721/2000], Loss: 0.0009\n",
      "Epoch [1722/2000], Loss: 0.0011\n",
      "Epoch [1723/2000], Loss: 0.0013\n",
      "Epoch [1724/2000], Loss: 0.0011\n",
      "Epoch [1725/2000], Loss: 0.0009\n",
      "Epoch [1726/2000], Loss: 0.0009\n",
      "Epoch [1727/2000], Loss: 0.0011\n",
      "Epoch [1728/2000], Loss: 0.0011\n",
      "Epoch [1729/2000], Loss: 0.0011\n",
      "Epoch [1730/2000], Loss: 0.0010\n",
      "Epoch [1731/2000], Loss: 0.0010\n",
      "Epoch [1732/2000], Loss: 0.0011\n",
      "Epoch [1733/2000], Loss: 0.0010\n",
      "Epoch [1734/2000], Loss: 0.0009\n",
      "Epoch [1735/2000], Loss: 0.0009\n",
      "Epoch [1736/2000], Loss: 0.0010\n",
      "Epoch [1737/2000], Loss: 0.0010\n",
      "Epoch [1738/2000], Loss: 0.0008\n",
      "Epoch [1739/2000], Loss: 0.0011\n",
      "Epoch [1740/2000], Loss: 0.0010\n",
      "Epoch [1741/2000], Loss: 0.0010\n",
      "Epoch [1742/2000], Loss: 0.0011\n",
      "Epoch [1743/2000], Loss: 0.0009\n",
      "Epoch [1744/2000], Loss: 0.0009\n",
      "Epoch [1745/2000], Loss: 0.0011\n",
      "Epoch [1746/2000], Loss: 0.0010\n",
      "Epoch [1747/2000], Loss: 0.0011\n",
      "Epoch [1748/2000], Loss: 0.0011\n",
      "Epoch [1749/2000], Loss: 0.0011\n",
      "Epoch [1750/2000], Loss: 0.0010\n",
      "Epoch [1751/2000], Loss: 0.0010\n",
      "Epoch [1752/2000], Loss: 0.0008\n",
      "Loss: 0.0008，效果更好，进行保存\n",
      "Epoch [1753/2000], Loss: 0.0011\n",
      "Epoch [1754/2000], Loss: 0.0010\n",
      "Epoch [1755/2000], Loss: 0.0011\n",
      "Epoch [1756/2000], Loss: 0.0010\n",
      "Epoch [1757/2000], Loss: 0.0011\n",
      "Epoch [1758/2000], Loss: 0.0011\n",
      "Epoch [1759/2000], Loss: 0.0010\n",
      "Epoch [1760/2000], Loss: 0.0010\n",
      "Epoch [1761/2000], Loss: 0.0010\n",
      "Epoch [1762/2000], Loss: 0.0011\n",
      "Epoch [1763/2000], Loss: 0.0010\n",
      "Epoch [1764/2000], Loss: 0.0010\n",
      "Epoch [1765/2000], Loss: 0.0011\n",
      "Epoch [1766/2000], Loss: 0.0011\n",
      "Epoch [1767/2000], Loss: 0.0009\n",
      "Epoch [1768/2000], Loss: 0.0010\n",
      "Epoch [1769/2000], Loss: 0.0011\n",
      "Epoch [1770/2000], Loss: 0.0009\n",
      "Epoch [1771/2000], Loss: 0.0010\n",
      "Epoch [1772/2000], Loss: 0.0009\n",
      "Epoch [1773/2000], Loss: 0.0011\n",
      "Epoch [1774/2000], Loss: 0.0010\n",
      "Epoch [1775/2000], Loss: 0.0011\n",
      "Epoch [1776/2000], Loss: 0.0010\n",
      "Epoch [1777/2000], Loss: 0.0010\n",
      "Epoch [1778/2000], Loss: 0.0011\n",
      "Epoch [1779/2000], Loss: 0.0010\n",
      "Epoch [1780/2000], Loss: 0.0010\n",
      "Epoch [1781/2000], Loss: 0.0010\n",
      "Epoch [1782/2000], Loss: 0.0010\n",
      "Epoch [1783/2000], Loss: 0.0010\n",
      "Epoch [1784/2000], Loss: 0.0012\n",
      "Epoch [1785/2000], Loss: 0.0010\n",
      "Epoch [1786/2000], Loss: 0.0008\n",
      "Loss: 0.0008，效果更好，进行保存\n",
      "Epoch [1787/2000], Loss: 0.0010\n",
      "Epoch [1788/2000], Loss: 0.0010\n",
      "Epoch [1789/2000], Loss: 0.0012\n",
      "Epoch [1790/2000], Loss: 0.0009\n",
      "Epoch [1791/2000], Loss: 0.0010\n",
      "Epoch [1792/2000], Loss: 0.0010\n",
      "Epoch [1793/2000], Loss: 0.0010\n",
      "Epoch [1794/2000], Loss: 0.0010\n",
      "Epoch [1795/2000], Loss: 0.0010\n",
      "Epoch [1796/2000], Loss: 0.0010\n",
      "Epoch [1797/2000], Loss: 0.0010\n",
      "Epoch [1798/2000], Loss: 0.0011\n",
      "Epoch [1799/2000], Loss: 0.0009\n",
      "Epoch [1800/2000], Loss: 0.0010\n",
      "Epoch [1801/2000], Loss: 0.0011\n",
      "Epoch [1802/2000], Loss: 0.0010\n",
      "Epoch [1803/2000], Loss: 0.0010\n",
      "Epoch [1804/2000], Loss: 0.0010\n",
      "Epoch [1805/2000], Loss: 0.0011\n",
      "Epoch [1806/2000], Loss: 0.0011\n",
      "Epoch [1807/2000], Loss: 0.0011\n",
      "Epoch [1808/2000], Loss: 0.0010\n",
      "Epoch [1809/2000], Loss: 0.0011\n",
      "Epoch [1810/2000], Loss: 0.0010\n",
      "Epoch [1811/2000], Loss: 0.0011\n",
      "Epoch [1812/2000], Loss: 0.0010\n",
      "Epoch [1813/2000], Loss: 0.0010\n",
      "Epoch [1814/2000], Loss: 0.0010\n",
      "Epoch [1815/2000], Loss: 0.0009\n",
      "Epoch [1816/2000], Loss: 0.0010\n",
      "Epoch [1817/2000], Loss: 0.0010\n",
      "Epoch [1818/2000], Loss: 0.0010\n",
      "Epoch [1819/2000], Loss: 0.0009\n",
      "Epoch [1820/2000], Loss: 0.0009\n",
      "Epoch [1821/2000], Loss: 0.0013\n",
      "Epoch [1822/2000], Loss: 0.0011\n",
      "Epoch [1823/2000], Loss: 0.0009\n",
      "Epoch [1824/2000], Loss: 0.0011\n",
      "Epoch [1825/2000], Loss: 0.0011\n",
      "Epoch [1826/2000], Loss: 0.0011\n",
      "Epoch [1827/2000], Loss: 0.0011\n",
      "Epoch [1828/2000], Loss: 0.0011\n",
      "Epoch [1829/2000], Loss: 0.0010\n",
      "Epoch [1830/2000], Loss: 0.0010\n",
      "Epoch [1831/2000], Loss: 0.0010\n",
      "Epoch [1832/2000], Loss: 0.0011\n",
      "Epoch [1833/2000], Loss: 0.0012\n",
      "Epoch [1834/2000], Loss: 0.0011\n",
      "Epoch [1835/2000], Loss: 0.0011\n",
      "Epoch [1836/2000], Loss: 0.0009\n",
      "Epoch [1837/2000], Loss: 0.0011\n",
      "Epoch [1838/2000], Loss: 0.0010\n",
      "Epoch [1839/2000], Loss: 0.0010\n",
      "Epoch [1840/2000], Loss: 0.0012\n",
      "Epoch [1841/2000], Loss: 0.0010\n",
      "Epoch [1842/2000], Loss: 0.0011\n",
      "Epoch [1843/2000], Loss: 0.0010\n",
      "Epoch [1844/2000], Loss: 0.0010\n",
      "Epoch [1845/2000], Loss: 0.0011\n",
      "Epoch [1846/2000], Loss: 0.0009\n",
      "Epoch [1847/2000], Loss: 0.0009\n",
      "Epoch [1848/2000], Loss: 0.0011\n",
      "Epoch [1849/2000], Loss: 0.0011\n",
      "Epoch [1850/2000], Loss: 0.0011\n",
      "Epoch [1851/2000], Loss: 0.0010\n",
      "Epoch [1852/2000], Loss: 0.0010\n",
      "Epoch [1853/2000], Loss: 0.0009\n",
      "Epoch [1854/2000], Loss: 0.0010\n",
      "Epoch [1855/2000], Loss: 0.0009\n",
      "Epoch [1856/2000], Loss: 0.0010\n",
      "Epoch [1857/2000], Loss: 0.0009\n",
      "Epoch [1858/2000], Loss: 0.0011\n",
      "Epoch [1859/2000], Loss: 0.0012\n",
      "Epoch [1860/2000], Loss: 0.0010\n",
      "Epoch [1861/2000], Loss: 0.0010\n",
      "Epoch [1862/2000], Loss: 0.0010\n",
      "Epoch [1863/2000], Loss: 0.0010\n",
      "Epoch [1864/2000], Loss: 0.0011\n",
      "Epoch [1865/2000], Loss: 0.0012\n",
      "Epoch [1866/2000], Loss: 0.0013\n",
      "Epoch [1867/2000], Loss: 0.0010\n",
      "Epoch [1868/2000], Loss: 0.0012\n",
      "Epoch [1869/2000], Loss: 0.0012\n",
      "Epoch [1870/2000], Loss: 0.0010\n",
      "Epoch [1871/2000], Loss: 0.0010\n",
      "Epoch [1872/2000], Loss: 0.0010\n",
      "Epoch [1873/2000], Loss: 0.0009\n",
      "Epoch [1874/2000], Loss: 0.0010\n",
      "Epoch [1875/2000], Loss: 0.0010\n",
      "Epoch [1876/2000], Loss: 0.0011\n",
      "Epoch [1877/2000], Loss: 0.0010\n",
      "Epoch [1878/2000], Loss: 0.0011\n",
      "Epoch [1879/2000], Loss: 0.0011\n",
      "Epoch [1880/2000], Loss: 0.0011\n",
      "Epoch [1881/2000], Loss: 0.0010\n",
      "Epoch [1882/2000], Loss: 0.0010\n",
      "Epoch [1883/2000], Loss: 0.0011\n",
      "Epoch [1884/2000], Loss: 0.0010\n",
      "Epoch [1885/2000], Loss: 0.0013\n",
      "Epoch [1886/2000], Loss: 0.0011\n",
      "Epoch [1887/2000], Loss: 0.0010\n",
      "Epoch [1888/2000], Loss: 0.0010\n",
      "Epoch [1889/2000], Loss: 0.0010\n",
      "Epoch [1890/2000], Loss: 0.0010\n",
      "Epoch [1891/2000], Loss: 0.0011\n",
      "Epoch [1892/2000], Loss: 0.0012\n",
      "Epoch [1893/2000], Loss: 0.0010\n",
      "Epoch [1894/2000], Loss: 0.0010\n",
      "Epoch [1895/2000], Loss: 0.0009\n",
      "Epoch [1896/2000], Loss: 0.0010\n",
      "Epoch [1897/2000], Loss: 0.0011\n",
      "Epoch [1898/2000], Loss: 0.0010\n",
      "Epoch [1899/2000], Loss: 0.0009\n",
      "Epoch [1900/2000], Loss: 0.0010\n",
      "Epoch [1901/2000], Loss: 0.0009\n",
      "Epoch [1902/2000], Loss: 0.0009\n",
      "Epoch [1903/2000], Loss: 0.0009\n",
      "Epoch [1904/2000], Loss: 0.0010\n",
      "Epoch [1905/2000], Loss: 0.0009\n",
      "Epoch [1906/2000], Loss: 0.0011\n",
      "Epoch [1907/2000], Loss: 0.0009\n",
      "Epoch [1908/2000], Loss: 0.0009\n",
      "Epoch [1909/2000], Loss: 0.0009\n",
      "Epoch [1910/2000], Loss: 0.0012\n",
      "Epoch [1911/2000], Loss: 0.0010\n",
      "Epoch [1912/2000], Loss: 0.0010\n",
      "Epoch [1913/2000], Loss: 0.0011\n",
      "Epoch [1914/2000], Loss: 0.0011\n",
      "Epoch [1915/2000], Loss: 0.0011\n",
      "Epoch [1916/2000], Loss: 0.0010\n",
      "Epoch [1917/2000], Loss: 0.0010\n",
      "Epoch [1918/2000], Loss: 0.0010\n",
      "Epoch [1919/2000], Loss: 0.0010\n",
      "Epoch [1920/2000], Loss: 0.0009\n",
      "Epoch [1921/2000], Loss: 0.0010\n",
      "Epoch [1922/2000], Loss: 0.0010\n",
      "Epoch [1923/2000], Loss: 0.0009\n",
      "Epoch [1924/2000], Loss: 0.0010\n",
      "Epoch [1925/2000], Loss: 0.0009\n",
      "Epoch [1926/2000], Loss: 0.0010\n",
      "Epoch [1927/2000], Loss: 0.0011\n",
      "Epoch [1928/2000], Loss: 0.0010\n",
      "Epoch [1929/2000], Loss: 0.0010\n",
      "Epoch [1930/2000], Loss: 0.0010\n",
      "Epoch [1931/2000], Loss: 0.0009\n",
      "Epoch [1932/2000], Loss: 0.0012\n",
      "Epoch [1933/2000], Loss: 0.0009\n",
      "Epoch [1934/2000], Loss: 0.0011\n",
      "Epoch [1935/2000], Loss: 0.0010\n",
      "Epoch [1936/2000], Loss: 0.0009\n",
      "Epoch [1937/2000], Loss: 0.0009\n",
      "Epoch [1938/2000], Loss: 0.0011\n",
      "Epoch [1939/2000], Loss: 0.0009\n",
      "Epoch [1940/2000], Loss: 0.0011\n",
      "Epoch [1941/2000], Loss: 0.0009\n",
      "Epoch [1942/2000], Loss: 0.0011\n",
      "Epoch [1943/2000], Loss: 0.0009\n",
      "Epoch [1944/2000], Loss: 0.0010\n",
      "Epoch [1945/2000], Loss: 0.0010\n",
      "Epoch [1946/2000], Loss: 0.0011\n",
      "Epoch [1947/2000], Loss: 0.0009\n",
      "Epoch [1948/2000], Loss: 0.0009\n",
      "Epoch [1949/2000], Loss: 0.0010\n",
      "Epoch [1950/2000], Loss: 0.0010\n"
     ]
    }
   ],
   "source": [
    "# 训练模型\n",
    "epochs = 2000\n",
    "losss = 15\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    for input_tensor, target_tensor in dataloader:\n",
    "        input_tensor = input_tensor.to(device)\n",
    "        target_tensor = target_tensor.to(device)\n",
    "        output = model(input_tensor)\n",
    "        loss = criterion(output.view(-1, output_size), target_tensor.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # if (epoch + 1) % 10 == 0:\n",
    "        #     print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\")\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\")\n",
    "    if loss.item() < losss:\n",
    "        losss = loss.item()\n",
    "        torch.save(model, 'model_seq2seq_h128.pth')\n",
    "        print(f\"Loss: {loss.item():.4f}，效果更好，进行保存\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5a1d47e3-c5d2-4c05-b79b-d361caf6aa10",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (embedding): Embedding(3865, 128)\n",
       "  (encoder): LSTM(128, 128, batch_first=True)\n",
       "  (decoder): LSTM(128, 128, batch_first=True)\n",
       "  (fc): Linear(in_features=128, out_features=3865, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.load('./model_seq2seq_h128.pth',weights_only=False,map_location=torch.device('cpu'))\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9fcd5190-3121-4d7c-a62d-a9251c634873",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test_met(input_text):\n",
    "    input_seq = [text_to_seq(text, char_to_idx) for text in [input_text]]\n",
    "    max_len = 10\n",
    "    input_seq = [seq +[1] + [0] * (max_len - len(seq) -1) for seq in input_seq]\n",
    "    input_seq = torch.tensor(input_seq)\n",
    "    input_seq = input_seq.to(device)\n",
    "    test_output = model(input_seq)\n",
    "    predicted = torch.argmax(test_output, dim=-1)\n",
    "    # print(f\"Predicted: {''.join([idx_to_char[idx.item()] for idx in predicted[0]])}\")\n",
    "    return {''.join([idx_to_char[idx.item()] for idx in predicted[0]])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "282eba02-ce80-4fc8-be8c-e08b634e6386",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "少年力学志须彊\n",
      "{'得失由来一梦长<PAD><EOS><EOS>'}\n",
      "得失由来一梦长\n",
      "__________________\n",
      "试问邯郸欹枕客\n",
      "{'人间几度熟黄粱<PAD><EOS><EOS>'}\n",
      "人间几度熟黄粱\n",
      "__________________\n",
      "脂脸轻匀作艳粧\n",
      "{'未应洁白似梅香<PAD><EOS><EOS>'}\n",
      "未应洁白似梅香\n",
      "__________________\n",
      "夭红不见凌霜操\n",
      "{'漫向春前取次芳<PAD><EOS><EOS>'}\n",
      "漫向春前取次芳\n",
      "__________________\n",
      "纷纷朝市竞秋毫\n",
      "{'江上霜风正怒号<PAD><EOS><EOS>'}\n",
      "江上霜风正怒号\n",
      "__________________\n",
      "不问扬澜与彭浪\n",
      "{'翩然东下日千艘<PAD><EOS><EOS>'}\n",
      "翩然东下日千艘\n",
      "__________________\n",
      "似闻疏雨打篷声\n",
      "{'枕上悠扬梦半醒<PAD><EOS><EOS>'}\n",
      "枕上悠扬梦半醒\n",
      "__________________\n",
      "明日觉来浑不记\n",
      "{'隔船相语过前汀<PAD><EOS><EOS>'}\n",
      "隔船相语过前汀\n",
      "__________________\n",
      "夹屋青松翠霭中\n",
      "{'去年经此亦匆匆<PAD><EOS><EOS>'}\n",
      "去年经此亦匆匆\n",
      "__________________\n",
      "重来乌石冈头路\n",
      "{'依旧松声带晓风<PAD><EOS><EOS>'}\n",
      "依旧松声带晓风\n",
      "__________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(input_texts[i])\n",
    "    print(test_met(input_texts[i]))\n",
    "    print(target_texts[i])\n",
    "    print('__________________')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ab534d81-4e7d-49c6-94dc-054a2d60e7cf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'老栖衣白龙九和<PAD><EOS><EOS>'}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_met('风急天高鸟飞回')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
